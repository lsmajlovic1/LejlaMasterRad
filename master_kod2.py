# -*- coding: utf-8 -*-
"""master_kod2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qWyj3gjbyvB2-lBOhasDW6IybFX02sO2
"""

import tensorflow as tf
tf.test.gpu_device_name()

!pip install wfdb

import os
import wfdb
import numpy as np
import matplotlib.pyplot as plt

#podaci
if os.path.isdir("afdb"): #AF database
    print('You already have the data.')
else:
    wfdb.dl_database('afdb', 'afdb')

def chunks(lst, n):
    
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

import pywt
# filtriranje šuma sa db4 (izvor: https://www.kaggle.com/code/theoviel/denoising-with-direct-wavelet-transform)
'''The denoising steps are the following :

1. Apply the dwt to the signal
2. Compute the threshold corresponding to the chosen level
3. Only keep coefficients with a value higher than the threshold
4. Apply the inverse dwt to retrieve the signal'''
# from statsmodels.robust import mad
def madev(d, axis=None): 
    """ Mean absolute deviation of a signal """
    return np.mean(np.absolute(d - np.mean(d, axis)), axis)
def wavelet_denoising(x, wavelet='db4', level=1):
    coeff = pywt.wavedec(x, wavelet, mode="per")
  #  print(madev(coeff[-level]))
    sigma = (1/0.6745) * madev(coeff[-level]) # sigma (st.devijacija)=k*MAD (faktor sklarianja) approx 1.4826 za normalnu distirbuciju)
   # print(sigma)
    uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])
    return pywt.waverec(coeff, wavelet, mode='per')

def get_ann(annotation, data):
  cat = np.array(annotation.aux_note)
  rate = np.zeros_like(cat, dtype='float')

  for catid, catval in enumerate(cat):
    #if (catval == '(N'):
     # rate[catid] = 1 # Normal
    if (catval in '(AFIB'):
      rate[catid] = 1 # AFIB

  rates = np.zeros(len(data))
#rates[annotation.sample] = rate
  i=0
  for x in annotation.sample:
  
    rates[x]=rate[i]
    i=i+1

  indices = np.arange(data[0].size, dtype='int')

  return rates

from scipy import signal

def get_beats(rpeaks, rates, beats):
  
  beatstoremove = np.array([0])
  '''
  for idx, idxval in enumerate(rpeaks):
    firstround = idx == 0
    lastround = idx == len(beats) - 1

    # Skip first and last beat.
    if (firstround or lastround):
      continue

                  # Get the classification value that is on
              # or near the position of the rpeak index.
    fromidx = 0 if idxval < 50 else idxval - 50
    toidx = idxval + 50
    catval = rates[fromidx:toidx].max()
        '''      
    # Skip beat if there is no classification.
    #if (catval == 0.0):
   #   beatstoremove = np.append(beatstoremove, idx)
 #     continue

              # Normal beat is now classified as 0.0 and abnormal is 1.0.
    #catval = catval - 1.0

              # Append some extra readings from next beat.
   # beats[idx] = np.append(beats[idx], beats[idx+1][:40])
   
    #poceti svaki otkucaj sa 2/3 od prethodnog RR intervala
   
  # Normalize the readings to a 0-1 range for ML purposes.
  for idx in range(len(beats)):
    firstround = idx == 0
    lastround = idx == len(beats) - 1

    # Skip first and last beat.
    if (firstround or lastround):
      continue

    catval=rates[idx].max()
    beats[idx] = (beats[idx] - beats[idx].min()) / beats[idx].ptp()

    # Resample from 360Hz to 125Hz
    newsize = int((beats[idx].size * 125 / 250) + 0.5)
    beats[idx] = signal.resample(beats[idx], newsize)
  # Skipping records that are too long.
    if (beats[idx].size > 187):
      beatstoremove = np.append(beatstoremove, idx)
      continue

              # Pad with zeroes.
    zerocount = 187 - beats[idx].size
    beats[idx] = np.pad(beats[idx], (0, zerocount), 'constant', constant_values=(0.0, 0.0))

              # Append the classification to the beat data.
    
    beats[idx] = np.append(beats[idx], catval)


  beatstoremove = np.append(beatstoremove, len(beats)-1)

          # Remove  beats 
  beats = np.delete(beats, beatstoremove)
  oznake=[]
  beats_split=[]
  for k,x in enumerate(beats):
    oznake.append(x[-1])
    beats_split.append(np.delete(x, -1))

  beats5=list(chunks(beats_split, 5))
  oznake5=list(chunks(oznake, 5))

  #lista anotacija 1 i 0
  oznake_5max=[]
  for x in oznake5:
    oznake_5max.append(max(x))

  if len(beats5)%5!=0 and max(oznake_5max[-5:-1])!=1:
    beats5=np.delete(beats5, -1)
    oznake_5max=np.delete(oznake_5max, -1)

  df_joined=[0] * (len(beats5))
  X=[0] * (len(beats5))
  for k,x in enumerate(beats5):
   X[k]=np.concatenate((x[0], x[1], x[2], x[3], x[4]))
   df_joined[k]=np.concatenate((x[0], x[1], x[2], x[3], x[4], [oznake_5max[k]]))

  return df_joined, X, oznake_5max

def get_annotation(fajlovi):
    s1='afdb/'
    s=s1+fajlovi[0]
    record=wfdb.rdrecord(s)
    data=record.p_signal[:,0]
    
    annotation= wfdb.rdann('afdb/04015', 'atr') 
    rates=get_ann(annotation, data)
    for x in fajlovi[1:]:
    
      s=s1+x
      #print(s)
      record=wfdb.rdrecord(s)
      data=record.p_signal[:,0]
      annotation= wfdb.rdann(s, 'atr') 
      rates=np.append(rates,get_ann(annotation, data))

       

    return rates

def get_data_ann(fajlovi):
    s1='afdb/'
    s=s1+fajlovi[0]
    record=wfdb.rdrecord(s)
    signal=record.p_signal[:,0]
    signal_db4=np.transpose(wavelet_denoising(signal, wavelet='db4', level=1))
    data=signal_db4
   # annotation= wfdb.rdann('afdb/04015', 'atr') 
   # rates=get_ann(annotation, data)
    for x in fajlovi[1:]:
    
      s=s1+x
      #print(s)
      record=wfdb.rdrecord(s)
      signal=record.p_signal[:,0]
      signal_db4=np.transpose(wavelet_denoising(signal, wavelet='db4', level=1))
      data=np.append(data,signal_db4)
    #  annotation= wfdb.rdann(s, 'atr') 
    #  rates=np.append(rates,get_ann(annotation, data))

    rates=get_annotation(fajlovi)

       

    return data, rates

fajlovi=['04015', '04043', '04048', '04126', '04746', '04908', '05121',
         '05261', '06426', '06995', '07162', '07859', '07879', '07910',
         '08215', '08219']

from wfdb import processing
import pandas as pd
data, rates=get_data_ann(fajlovi)
#(pd.DataFrame(data)).to_csv('data.csv')

#(pd.DataFrame(rates)).to_csv('annotations.csv')

counts=np.unique(rates, return_counts=True)
print(counts)

#rpeaks= processing.xqrs_detect(sig=data, fs=250)
from google.colab import files
import pandas as pd
rpeaks=pd.read_csv("rpeaks.csv")
rpeaks_np=rpeaks.to_numpy()
beats_RR=np.split(data, rpeaks_np[:,1])
RR_23=(np.floor(np.diff(rpeaks_np[:,1])*2/3)).astype(np.int64)
sume=np.cumsum(RR_23)
beats_23=np.split(data, sume)
rates_23=np.split(rates, sume)
#len(rates_23)==len(beats_23)

#plt.plot(beats_23[13])

#provjera 2/3 RR intervala
'''
from operator import itemgetter 
l1=data[0:10000]
l2=sume[0:50]
l=[]
for i in l2:
  l.append(l1[i])

plt.figure(figsize=(40, 15))
plt.plot(l1[0:5500])
plt.scatter(l2,l, color='r') '''

def get_list(a):
  lista=[]
  for x in a:
    lista.append(list(x))

  return lista

df_joined, X, oznake_5max =get_beats(rpeaks_np[:,1], rates_23, beats_23)

#(pd.DataFrame(X)).r_d.to_csv('X.csv')
#(pd.DataFrame(oznake_5max)).r_d.to_csv('y.csv')

#checking imbalance of the labels
from collections import Counter
counter_before = Counter(oznake_5max)
print(counter_before)
#potrebno je da broj 1 i 0 bude izjednačen
import seaborn as sns
plt.figure(figsize=(8, 8))
plt.bar(counter_before.keys(), counter_before.values())
plt.title('Neizbalansirane klase')
plt.show()

# svaku od klasa postaviti da ima jednak broj - npr 50 000
# klasu 0 smanjiti broj
# klasu 1 povećati broj

#applying SMOTE for imbalance
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
lista=list(counter_before.values())
# define pipeline
over = SMOTE(sampling_strategy= {0: lista[0], 1: 1000}) #crasha za sve 
under = RandomUnderSampler(sampling_strategy=1)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)

# transform the dataset
X, y = pipeline.fit_resample(X, oznake_5max)
# summarize the new class distribution
counter_after = Counter(y)
print(counter_after)
import seaborn as sns
plt.figure(figsize=(8, 8))
plt.bar(counter_after.keys(), counter_after.values())
plt.title('Neizbalansirane klase')
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.25, random_state=42)

#coef, freqs = pywt.cwt(x, scales, 'gaus4')  # Finding CWT using gaussian wavelet
#ECG1_db5=pywt.idwt(ECG1_db4, None, 'db5')
#wavelet = "mexh"  # mexh, morl, gaus8, gaus4
 # scales = pywt.central_frequency(wavelet) * sampling_rate / np.arange(1, 101, 1)
#pywt.cwt(data["signal"], scales, wavelet, sampling_period)

